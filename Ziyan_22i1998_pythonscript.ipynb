{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3fbea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "############connect with mysql db\n",
    "###ignore this cell please\n",
    "from sqlalchemy import create_engine, text\n",
    "USER = \"root\"\n",
    "#enter your pass here\n",
    "PASSWORD = \"\"\n",
    "HOST = \"localhost\"\n",
    "PORT = 3306\n",
    "DB = \"walmart_fin\"\n",
    "engine = create_engine(f\"mysql+pymysql://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}?charset=utf8mb4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start from here\n",
    "from sqlalchemy import create_engine, text\n",
    "print(\"Enter MySQL database credentials:\")\n",
    "USER = input(\"Username: \")\n",
    "PASSWORD = input(\"Password: \")\n",
    "HOST = input(\"Host (default: localhost): \") or \"localhost\"\n",
    "PORT = input(\"Port (default: 3306): \") or \"3306\"\n",
    "DB = \"walmart_fin\"\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}?charset=utf8mb4\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e4a612f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting data from csv and xlsx files.\n",
      "extracted customer data\n",
      "extracted product data\n",
      "extract transactional data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#####extracting data from csvs\n",
    "print(\"extracting data from csv and xlsx files.\")\n",
    "cust_df = pd.read_excel(\"customer_master_data.xlsx\")\n",
    "print(\"extracted customer data\")\n",
    "prod_df = pd.read_excel(\"product_master_data.xlsx\")\n",
    "print(\"extracted product data\")\n",
    "trans_df = pd.read_csv(\"transactional_data.csv\", parse_dates=[\"date\"])\n",
    "print(\"extract transactional data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5ffeeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transforming and cleaning data...\n",
      "Cleaning and normalizing...\n",
      " After cleaning — Customers: 5891, Products: 3631, Transactions: 550068\n",
      "Building dimensions...\n",
      " Transformation completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_8028\\2730124231.py:66: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  prod_df[\"unit_price\"].fillna(prod_df[\"unit_price\"].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_8028\\2730124231.py:67: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  trans_df[\"quantity\"].fillna(1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\" Transforming and cleaning data...\")\n",
    "# STEP 1 RENAME COLUMNS (Normalization of Schema)\n",
    "# Customer dimension\n",
    "cust_df.rename(columns={\n",
    "    \"Customer_ID\": \"customer_id\",\n",
    "    \"Gender\": \"gender\",\n",
    "    \"Age\": \"age\",\n",
    "    \"Occupation\": \"occupation\",\n",
    "    \"City_Category\": \"city_category\",\n",
    "    \"Stay_In_Current_City_Years\": \"stay_in_current_city_years\",\n",
    "    \"Marital_Status\": \"marital_status\"\n",
    "}, inplace=True)\n",
    "\n",
    "cust_df[\"age_group\"] = cust_df[\"age\"]\n",
    "# Product + store + supplier dimension\n",
    "prod_df.rename(columns={\n",
    "    \"Product_ID\": \"product_id\",\n",
    "    \"Product_Category\": \"product_category\",\n",
    "    \"price$\": \"unit_price\",\n",
    "    \"storeID\": \"store_id\",\n",
    "    \"supplierID\": \"supplier_id\",\n",
    "    \"storeName\": \"store_name\",\n",
    "    \"supplierName\": \"supplier_name\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Transactional data\n",
    "trans_df.rename(columns={\n",
    "    \"orderID\": \"order_id\",\n",
    "    \"Customer_ID\": \"customer_id\",\n",
    "    \"Product_ID\": \"product_id\",\n",
    "    \"quantity\": \"quantity\",\n",
    "    \"date\": \"date_id\"\n",
    "}, inplace=True)\n",
    "\n",
    "print(\"Cleaning and normalizing...\")\n",
    "\n",
    "# Encoding conversion\n",
    "cust_df[\"gender\"] = cust_df[\"gender\"].replace({\"M\": \"Male\", \"F\": \"Female\"})\n",
    "cust_df[\"marital_status\"] = cust_df[\"marital_status\"].replace({0: \"Single\", 1: \"Married\"})\n",
    "\n",
    "# Text normalization\n",
    "cust_df[\"city_category\"] = cust_df[\"city_category\"].str.upper().str.strip()\n",
    "prod_df[\"product_category\"] = prod_df[\"product_category\"].str.title().str.strip()\n",
    "prod_df[\"store_name\"] = prod_df[\"store_name\"].str.title().str.strip()\n",
    "prod_df[\"supplier_name\"] = prod_df[\"supplier_name\"].str.title().str.strip()\n",
    "\n",
    "# Convert numeric and date fields\n",
    "prod_df[\"unit_price\"] = pd.to_numeric(prod_df[\"unit_price\"], errors=\"coerce\")\n",
    "trans_df[\"quantity\"] = pd.to_numeric(trans_df[\"quantity\"], errors=\"coerce\")\n",
    "trans_df[\"date_id\"] = pd.to_datetime(trans_df[\"date_id\"], errors=\"coerce\")\n",
    "\n",
    "# Remove duplicates\n",
    "cust_df.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "prod_df.drop_duplicates(subset=[\"product_id\"], inplace=True)\n",
    "trans_df.drop_duplicates(subset=[\"order_id\"], inplace=True)\n",
    "\n",
    "# Handle missing values\n",
    "cust_df.fillna({\n",
    "    \"gender\": \"Unknown\",\n",
    "    \"city_category\": \"Unknown\",\n",
    "    \"age\": \"Unknown\"\n",
    "}, inplace=True)\n",
    "\n",
    "prod_df[\"unit_price\"].fillna(prod_df[\"unit_price\"].median(), inplace=True)\n",
    "trans_df[\"quantity\"].fillna(1, inplace=True)\n",
    "\n",
    "# Remove rows with missing key references\n",
    "trans_df.dropna(subset=[\"customer_id\", \"product_id\"], inplace=True)\n",
    "\n",
    "# Consistency checks\n",
    "invalid_rows = trans_df[trans_df[\"quantity\"] <= 0]\n",
    "if not invalid_rows.empty:\n",
    "    print(f\" Found {len(invalid_rows)} invalid quantity rows — removing them.\")\n",
    "    trans_df = trans_df[trans_df[\"quantity\"] > 0]\n",
    "\n",
    "print(f\" After cleaning — Customers: {len(cust_df)}, Products: {len(prod_df)}, Transactions: {len(trans_df)}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Building dimensions...\")\n",
    "\n",
    "# Date dimension\n",
    "dim_date = pd.DataFrame()\n",
    "dim_date[\"date_id\"] = pd.to_datetime(trans_df[\"date_id\"].unique())\n",
    "dim_date[\"day_of_week\"] = dim_date[\"date_id\"].dt.weekday + 1\n",
    "dim_date[\"is_weekend\"] = dim_date[\"day_of_week\"].isin([6, 7]).astype(int)\n",
    "dim_date[\"week_of_year\"] = dim_date[\"date_id\"].dt.isocalendar().week\n",
    "dim_date[\"day_of_month\"] = dim_date[\"date_id\"].dt.day\n",
    "dim_date[\"month\"] = dim_date[\"date_id\"].dt.month\n",
    "dim_date[\"month_name\"] = dim_date[\"date_id\"].dt.month_name()\n",
    "dim_date[\"quarter\"] = dim_date[\"date_id\"].dt.quarter\n",
    "dim_date[\"year\"] = dim_date[\"date_id\"].dt.year\n",
    "dim_date[\"season\"] = dim_date[\"month\"].map({\n",
    "    12: \"Winter\", 1: \"Winter\", 2: \"Winter\",\n",
    "    3: \"Spring\", 4: \"Spring\", 5: \"Spring\",\n",
    "    6: \"Summer\", 7: \"Summer\", 8: \"Summer\",\n",
    "    9: \"Fall\", 10: \"Fall\", 11: \"Fall\"\n",
    "})\n",
    "dim_date[\"is_holiday\"] = 0  #can extend later\n",
    "\n",
    "# Other dimensions\n",
    "dim_store = prod_df[[\"store_id\", \"store_name\"]].drop_duplicates()\n",
    "dim_supplier = prod_df[[\"supplier_id\", \"supplier_name\"]].drop_duplicates()\n",
    "dim_product = prod_df[[\"product_id\", \"product_category\", \"unit_price\", \"store_id\", \"supplier_id\"]].drop_duplicates()\n",
    "dim_customer = cust_df.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" Transformation completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "961f43d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_sales = pd.DataFrame()\n",
    "\n",
    "fact_sales[\"order_id\"] = trans_df[\"order_id\"]\n",
    "fact_sales[\"customer_id\"] = trans_df[\"customer_id\"]\n",
    "fact_sales[\"product_id\"] = trans_df[\"product_id\"]\n",
    "fact_sales[\"date_id\"] = pd.to_datetime(trans_df[\"date_id\"])\n",
    "fact_sales[\"quantity\"] = trans_df[\"quantity\"]\n",
    "\n",
    "# These columns will be filled later by HybridJoin\n",
    "fact_sales[\"unit_price\"] = None\n",
    "fact_sales[\"total_amount\"] = None\n",
    "fact_sales[\"store_id\"] = None\n",
    "fact_sales[\"supplier_id\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84a34f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_customer,making dfs for loading(for ease only)\n",
    "dim_customer = cust_df[[\n",
    "    \"customer_id\",\n",
    "    \"gender\",\n",
    "    \"age\",\n",
    "    \"age_group\",\n",
    "    \"occupation\",\n",
    "    \"city_category\",\n",
    "    \"stay_in_current_city_years\",\n",
    "    \"marital_status\"\n",
    "]].drop_duplicates()\n",
    "dim_product = prod_df[[\n",
    "    \"product_id\",\n",
    "    \"product_category\",\n",
    "    \"unit_price\"\n",
    "]].drop_duplicates()\n",
    "dim_store = prod_df[[\n",
    "    \"store_id\",\n",
    "    \"store_name\"\n",
    "]].drop_duplicates()\n",
    "dim_supplier = prod_df[[\n",
    "    \"supplier_id\",\n",
    "    \"supplier_name\"\n",
    "]].drop_duplicates()\n",
    "dim_date = dim_date.drop_duplicates(subset=[\"date_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3405c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting data into sql\n",
      "Inserted dim_customer\n",
      "Inserted dim_store\n",
      "Inserted dim_supplier\n",
      "Inserted dim_product\n",
      "Inserted dim_date\n",
      "inserting empty fact sales\n",
      "insterted\n"
     ]
    }
   ],
   "source": [
    "#loading into dim tables in sqlll\n",
    "print(\"inserting data into sql\")\n",
    "\n",
    "dim_customer.to_sql(\"dim_customer\", engine, if_exists=\"append\", index=False)\n",
    "print(\"Inserted dim_customer\")\n",
    "\n",
    "dim_store.to_sql(\"dim_store\", engine, if_exists=\"append\", index=False)\n",
    "print(\"Inserted dim_store\")\n",
    "\n",
    "dim_supplier.to_sql(\"dim_supplier\", engine, if_exists=\"append\", index=False)\n",
    "print(\"Inserted dim_supplier\")\n",
    "\n",
    "dim_product.to_sql(\"dim_product\", engine, if_exists=\"append\", index=False)\n",
    "print(\"Inserted dim_product\")\n",
    "\n",
    "dim_date.to_sql(\"dim_date\", engine, if_exists=\"append\", index=False)\n",
    "print(\"Inserted dim_date\")\n",
    "\n",
    "print(\"inserting empty fact sales\")#raw trans only\n",
    "\n",
    "print(\"insterted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1837fb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted raw fact_sales\n"
     ]
    }
   ],
   "source": [
    "fact_raw = trans_df[[\"order_id\", \"customer_id\", \"product_id\", \"date_id\", \"quantity\"]].copy()\n",
    "fact_raw.to_sql(\"fact_sales\", engine, if_exists=\"append\", index=False)\n",
    "print(\"Inserted raw fact_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4c4535f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading product master data (R)...\n",
      "Loaded 3631 products into master data\n",
      "Creating fact_sales_enriched table...\n",
      "Created fact_sales_enriched table\n",
      "============================================================\n",
      "Starting HYBRIDJOIN with Streaming\n",
      "============================================================\n",
      "Starting stream producer thread...\n",
      "Producer: Streamed 5000/550068 records\n",
      "Producer: Streamed 10000/550068 records\n",
      "HYBRIDJOIN thread started, waiting for data...\n",
      "Producer: Streamed 15000/550068 records\n",
      "Producer: Streamed 20000/550068 records\n",
      "Inserted 7677 records | Total joined: 7677\n",
      "Producer: Streamed 25000/550068 records\n",
      "Inserted 5196 records | Total joined: 12873\n",
      "Producer: Streamed 30000/550068 records\n",
      "Producer: Streamed 35000/550068 records\n",
      "Producer: Streamed 40000/550068 records\n",
      "Inserted 7159 records | Total joined: 20032\n",
      "Producer: Streamed 45000/550068 records\n",
      "Producer: Streamed 50000/550068 records\n",
      "Inserted 6361 records | Total joined: 26393\n",
      "Producer: Streamed 55000/550068 records\n",
      "Producer: Streamed 60000/550068 recordsInserted 7080 records | Total joined: 33473\n",
      "\n",
      "Producer: Streamed 65000/550068 records\n",
      "Producer: Streamed 70000/550068 records\n",
      "Inserted 6344 records | Total joined: 39817\n",
      "Producer: Streamed 75000/550068 records\n",
      "Producer: Streamed 80000/550068 records\n",
      "Inserted 7194 records | Total joined: 47011\n",
      "Producer: Streamed 85000/550068 records\n",
      "Producer: Streamed 90000/550068 records\n",
      "Inserted 5970 records | Total joined: 52981\n",
      "Producer: Streamed 95000/550068 records\n",
      "Producer: Streamed 100000/550068 records\n",
      "Inserted 7031 records | Total joined: 60012\n",
      "Producer: Streamed 105000/550068 records\n",
      "Producer: Streamed 110000/550068 records\n",
      "Producer: Streamed 115000/550068 records\n",
      "Inserted 6508 records | Total joined: 66520\n",
      "Producer: Streamed 120000/550068 records\n",
      "Producer: Streamed 125000/550068 records\n",
      "Inserted 6961 records | Total joined: 73481\n",
      "Producer: Streamed 130000/550068 records\n",
      "Inserted 6515 records | Total joined: 79996\n",
      "Producer: Streamed 135000/550068 records\n",
      "Producer: Streamed 140000/550068 records\n",
      "Inserted 6828 records | Total joined: 86824\n",
      "Producer: Streamed 145000/550068 records\n",
      "Producer: Streamed 150000/550068 records\n",
      "Inserted 6387 records | Total joined: 93211\n",
      "Producer: Streamed 155000/550068 records\n",
      "Producer: Streamed 160000/550068 records\n",
      "Inserted 6915 records | Total joined: 100126\n",
      "Producer: Streamed 165000/550068 records\n",
      "Inserted 6502 records | Total joined: 106628\n",
      "Producer: Streamed 170000/550068 records\n",
      "Producer: Streamed 175000/550068 records\n",
      "Inserted 6865 records | Total joined: 113493\n",
      "Producer: Streamed 180000/550068 records\n",
      "Producer: Streamed 185000/550068 records\n",
      "Inserted 6616 records | Total joined: 120109\n",
      "Producer: Streamed 190000/550068 records\n",
      "Inserted 6897 records | Total joined: 127006\n",
      "Producer: Streamed 195000/550068 records\n",
      "Producer: Streamed 200000/550068 records\n",
      "Inserted 6269 records | Total joined: 133275\n",
      "Producer: Streamed 205000/550068 records\n",
      "Producer: Streamed 210000/550068 records\n",
      "Inserted 6631 records | Total joined: 139906\n",
      "Producer: Streamed 215000/550068 records\n",
      "Producer: Streamed 220000/550068 records\n",
      "Inserted 6721 records | Total joined: 146627\n",
      "Producer: Streamed 225000/550068 records\n",
      "Producer: Streamed 230000/550068 records\n",
      "Inserted 6585 records | Total joined: 153212\n",
      "Producer: Streamed 235000/550068 records\n",
      "Producer: Streamed 240000/550068 records\n",
      "Inserted 6745 records | Total joined: 159957\n",
      "Producer: Streamed 245000/550068 records\n",
      "Producer: Streamed 250000/550068 records\n",
      "Inserted 6725 records | Total joined: 166682\n",
      "Producer: Streamed 255000/550068 records\n",
      "Producer: Streamed 260000/550068 records\n",
      "Inserted 6262 records | Total joined: 172944\n",
      "Producer: Streamed 265000/550068 records\n",
      "Producer: Streamed 270000/550068 records\n",
      "Inserted 7207 records | Total joined: 180151\n",
      "Producer: Streamed 275000/550068 records\n",
      "Inserted 6290 records | Total joined: 186441\n",
      "Producer: Streamed 280000/550068 records\n",
      "Producer: Streamed 285000/550068 records\n",
      "Producer: Streamed 290000/550068 records\n",
      "Inserted 6965 records | Total joined: 193406\n",
      "Producer: Streamed 295000/550068 records\n",
      "Producer: Streamed 300000/550068 records\n",
      "Inserted 6627 records | Total joined: 200033\n",
      "Producer: Streamed 305000/550068 records\n",
      "Inserted 6990 records | Total joined: 207023\n",
      "Producer: Streamed 310000/550068 records\n",
      "Producer: Streamed 315000/550068 records\n",
      "Inserted 6248 records | Total joined: 213271\n",
      "Producer: Streamed 320000/550068 records\n",
      "Inserted 6931 records | Total joined: 220202\n",
      "Producer: Streamed 325000/550068 records\n",
      "Producer: Streamed 330000/550068 records\n",
      "Inserted 6441 records | Total joined: 226643\n",
      "Producer: Streamed 335000/550068 records\n",
      "Producer: Streamed 340000/550068 records\n",
      "Inserted 6785 records | Total joined: 233428\n",
      "Producer: Streamed 345000/550068 records\n",
      "Producer: Streamed 350000/550068 records\n",
      "Inserted 6764 records | Total joined: 240192\n",
      "Producer: Streamed 355000/550068 records\n",
      "Producer: Streamed 360000/550068 records\n",
      "Producer: Streamed 365000/550068 records\n",
      "Inserted 6686 records | Total joined: 246878\n",
      "Producer: Streamed 370000/550068 records\n",
      "Producer: Streamed 375000/550068 records\n",
      "Inserted 6472 records | Total joined: 253350\n",
      "Producer: Streamed 380000/550068 records\n",
      "Producer: Streamed 385000/550068 records\n",
      "Inserted 6793 records | Total joined: 260143\n",
      "Producer: Streamed 390000/550068 records\n",
      "Producer: Streamed 395000/550068 records\n",
      "Inserted 6686 records | Total joined: 266829\n",
      "Producer: Streamed 400000/550068 records\n",
      "Producer: Streamed 405000/550068 records\n",
      "Producer: Streamed 410000/550068 records\n",
      "Inserted 6822 records | Total joined: 273651\n",
      "Producer: Streamed 415000/550068 records\n",
      "Producer: Streamed 420000/550068 recordsInserted 6631 records | Total joined: 280282\n",
      "\n",
      "Producer: Streamed 425000/550068 records\n",
      "Inserted 6351 records | Total joined: 286633\n",
      "Producer: Streamed 430000/550068 records\n",
      "Producer: Streamed 435000/550068 records\n",
      "Inserted 6516 records | Total joined: 293149\n",
      "Producer: Streamed 440000/550068 records\n",
      "Inserted 7084 records | Total joined: 300233\n",
      "Producer: Streamed 445000/550068 records\n",
      "Producer: Streamed 450000/550068 records\n",
      "Inserted 6555 records | Total joined: 306788\n",
      "Producer: Streamed 455000/550068 records\n",
      "Producer: Streamed 460000/550068 records\n",
      "Inserted 6879 records | Total joined: 313667\n",
      "Producer: Streamed 465000/550068 records\n",
      "Producer: Streamed 470000/550068 records\n",
      "Inserted 6649 records | Total joined: 320316\n",
      "Producer: Streamed 475000/550068 records\n",
      "Inserted 6408 records | Total joined: 326724\n",
      "Producer: Streamed 480000/550068 records\n",
      "Iter 100: Loaded=3436, Processed=339747, HashTable=2246, Queue=9587, Buffer=140478, StreamActive=True\n",
      "Producer: Streamed 485000/550068 records\n",
      "Producer: Streamed 490000/550068 records\n",
      "Inserted 6792 records | Total joined: 333516\n",
      "Producer: Streamed 495000/550068 records\n",
      "Producer: Streamed 500000/550068 records\n",
      "Inserted 6784 records | Total joined: 340300\n",
      "Producer: Streamed 505000/550068 records\n",
      "Producer: Streamed 510000/550068 records\n",
      "Inserted 6876 records | Total joined: 347176\n",
      "Producer: Streamed 515000/550068 records\n",
      "Producer: Streamed 520000/550068 records\n",
      "Inserted 6683 records | Total joined: 353859\n",
      "Producer: Streamed 525000/550068 records\n",
      "Producer: Streamed 530000/550068 records\n",
      "Inserted 6820 records | Total joined: 360679\n",
      "Producer: Streamed 535000/550068 records\n",
      "Producer: Streamed 540000/550068 records\n",
      "Inserted 6359 records | Total joined: 367038\n",
      "Producer: Streamed 545000/550068 records\n",
      "Inserted 6769 records | Total joined: 373807Producer: Streamed 550000/550068 records\n",
      "Producer: Finished streaming all 550068 records\n",
      "\n",
      "Inserted 6426 records | Total joined: 380233\n",
      "Inserted 6739 records | Total joined: 386972\n",
      "Inserted 6626 records | Total joined: 393598\n",
      "Inserted 6810 records | Total joined: 400408\n",
      "Inserted 6301 records | Total joined: 406709\n",
      "Inserted 6925 records | Total joined: 413634\n",
      "Inserted 6379 records | Total joined: 420013\n",
      "Inserted 6954 records | Total joined: 426967\n",
      "Inserted 6654 records | Total joined: 433621\n",
      "Inserted 6826 records | Total joined: 440447\n",
      "Inserted 6203 records | Total joined: 446650\n",
      "Inserted 7005 records | Total joined: 453655\n",
      "Inserted 6301 records | Total joined: 459956\n",
      "Inserted 6971 records | Total joined: 466927\n",
      "Inserted 6707 records | Total joined: 473634\n",
      "Inserted 6791 records | Total joined: 480425\n",
      "Inserted 6286 records | Total joined: 486711\n",
      "Inserted 7209 records | Total joined: 493920\n",
      "Inserted 6237 records | Total joined: 500157\n",
      "Inserted 6844 records | Total joined: 507001\n",
      "Inserted 6900 records | Total joined: 513901\n",
      "Inserted 6417 records | Total joined: 520318\n",
      "Inserted 6411 records | Total joined: 526729\n",
      "Inserted 6869 records | Total joined: 533598\n",
      "Inserted 6346 records | Total joined: 539944\n",
      "Inserted 5724 records | Total joined: 545668\n",
      "Queue empty and stream finished - exiting\n",
      "Inserted final 4400 records\n",
      "\n",
      "============================================================\n",
      "HYBRIDJOIN COMPLETED!\n",
      "Processed: 550068, Joined: 550068\n",
      "============================================================\n",
      "\n",
      "All operations completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# hybrid join  parameters\n",
    "hS = 10000\n",
    "vP = 500\n",
    "\n",
    "# storage dataa structures\n",
    "stream_buffer = queue.Queue()\n",
    "hash_table = {}\n",
    "join_queue = deque()\n",
    "w = hS\n",
    "stream_active = True\n",
    "\n",
    "# Loadingg master data\n",
    "print(\"Loading product master data (R)...\")\n",
    "prod_master_dict = prod_df[[\"product_id\", \"store_id\", \"supplier_id\", \"unit_price\"]].drop_duplicates()\n",
    "prod_master_dict = prod_master_dict.set_index(\"product_id\").to_dict(\"index\")\n",
    "all_product_keys = list(prod_master_dict.keys())\n",
    "print(f\"Loaded {len(prod_master_dict)} products into master data\")\n",
    "\n",
    "# Creating (in case i didnt do it in sql) enriched table\n",
    "print(\"Creating fact_sales_enriched table\")\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS fact_sales_enriched\"))\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE TABLE fact_sales_enriched (\n",
    "            order_id INT,\n",
    "            customer_id INT,\n",
    "            product_id VARCHAR(32),\n",
    "            date_id DATE,\n",
    "            quantity INT,\n",
    "            unit_price DECIMAL(10,2),\n",
    "            store_id INT,\n",
    "            supplier_id INT,\n",
    "            total_amount DECIMAL(10,2)\n",
    "        )\n",
    "    \"\"\"))\n",
    "    conn.commit()\n",
    "print(\"Created fact_sales_enriched table\")\n",
    "\n",
    "# Streaming thread\n",
    "def stream_producer():\n",
    "    global stream_active\n",
    "    print(\"Starting stream producer thread...\")\n",
    "    \n",
    "    trans_stream = pd.read_csv(\"transactional_data.csv\", parse_dates=[\"date\"])\n",
    "    trans_stream.rename(columns={\n",
    "        \"orderID\": \"order_id\",\n",
    "        \"Customer_ID\": \"customer_id\",\n",
    "        \"Product_ID\": \"product_id\",\n",
    "        \"quantity\": \"quantity\",\n",
    "        \"date\": \"date_id\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    total_rows = len(trans_stream)\n",
    "    for idx, row in trans_stream.iterrows():\n",
    "        stream_buffer.put(row.to_dict())\n",
    "        if (idx + 1) % 5000 == 0:\n",
    "            print(f\"Producer: Streamed {idx + 1}/{total_rows} records\")\n",
    "    \n",
    "    print(f\"Producer: Finished streaming all {total_rows} records\")\n",
    "    stream_active = False\n",
    "\n",
    "# HYBRIDJOIN algorithm\n",
    "def hybrid_join():\n",
    "    global w, hash_table, join_queue\n",
    "    \n",
    "    print(\"HYBRIDJOIN thread started, waiting for data!\")\n",
    "    processed_count = 0\n",
    "    joined_count = 0\n",
    "    iteration = 0\n",
    "    batch_to_insert = []\n",
    "    \n",
    "    while True:\n",
    "        iteration += 1\n",
    "        \n",
    "        # STEP 1: Load up to w tuples from stream buffer\n",
    "        loaded_count = 0\n",
    "        while loaded_count < w and not stream_buffer.empty():\n",
    "            try:\n",
    "                stream_tuple = stream_buffer.get_nowait()\n",
    "                join_key = stream_tuple[\"product_id\"]\n",
    "                \n",
    "                if join_key not in hash_table:\n",
    "                    hash_table[join_key] = []\n",
    "                hash_table[join_key].append(stream_tuple)\n",
    "                join_queue.append(join_key)\n",
    "                \n",
    "                loaded_count += 1\n",
    "                processed_count += 1\n",
    "                \n",
    "            except queue.Empty:\n",
    "                break\n",
    "        \n",
    "        w = 0\n",
    "        \n",
    "        # Debug output every iteration\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iter {iteration}: Loaded={loaded_count}, Processed={processed_count}, \"\n",
    "                  f\"HashTable={len(hash_table)}, Queue={len(join_queue)}, \"\n",
    "                  f\"Buffer={stream_buffer.qsize()}, StreamActive={stream_active}\")\n",
    "        \n",
    "        # Exit condition\n",
    "        if len(join_queue) == 0:\n",
    "            if not stream_active and stream_buffer.empty():\n",
    "                print(\"Queue empty and stream finished , exiting\")\n",
    "                break\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "        \n",
    "        # STEP 2: Get oldest key\n",
    "        oldest_key = join_queue[0]\n",
    "        \n",
    "        # STEP 3: Load disk partition - SIMPLIFIED\n",
    "        # Just load all products matching keys in current hash table\n",
    "        disk_buffer = []\n",
    "        unique_keys_in_hash = list(hash_table.keys())[:vP]  # Take first vP keys\n",
    "        \n",
    "        for pid in unique_keys_in_hash:\n",
    "            if pid in prod_master_dict:\n",
    "                disk_buffer.append((pid, prod_master_dict[pid]))\n",
    "        \n",
    "        # STEP 4: Probe hash table\n",
    "        matched_keys = set()\n",
    "        for disk_key, disk_data in disk_buffer:\n",
    "            if disk_key in hash_table:\n",
    "                for stream_tuple in hash_table[disk_key]:\n",
    "                    enriched = {\n",
    "                        \"order_id\": stream_tuple[\"order_id\"],\n",
    "                        \"customer_id\": stream_tuple[\"customer_id\"],\n",
    "                        \"product_id\": stream_tuple[\"product_id\"],\n",
    "                        \"date_id\": stream_tuple[\"date_id\"],\n",
    "                        \"quantity\": stream_tuple[\"quantity\"],\n",
    "                        \"unit_price\": disk_data[\"unit_price\"],\n",
    "                        \"store_id\": disk_data[\"store_id\"],\n",
    "                        \"supplier_id\": disk_data[\"supplier_id\"],\n",
    "                        \"total_amount\": stream_tuple[\"quantity\"] * disk_data[\"unit_price\"]\n",
    "                    }\n",
    "                    batch_to_insert.append(enriched)\n",
    "                    joined_count += 1\n",
    "                \n",
    "                num_deleted = len(hash_table[disk_key])\n",
    "                matched_keys.add(disk_key)\n",
    "                del hash_table[disk_key]\n",
    "                w += num_deleted\n",
    "        \n",
    "        # Remove matched keys from queue\n",
    "        if matched_keys:\n",
    "            join_queue = deque([k for k in join_queue if k not in matched_keys])\n",
    "        \n",
    "        # STEP 5: Batch insert\n",
    "        if len(batch_to_insert) >= 5000:\n",
    "            df_enriched = pd.DataFrame(batch_to_insert)\n",
    "            df_enriched[\"date_id\"] = pd.to_datetime(df_enriched[\"date_id\"])\n",
    "            df_enriched.to_sql(\"fact_sales_enriched\", engine, if_exists=\"append\", index=False)\n",
    "            print(f\"Inserted {len(batch_to_insert)} records | Total joined: {joined_count}\")\n",
    "            batch_to_insert = []\n",
    "    \n",
    "    # Final insert\n",
    "    if batch_to_insert:\n",
    "        df_enriched = pd.DataFrame(batch_to_insert)\n",
    "        df_enriched[\"date_id\"] = pd.to_datetime(df_enriched[\"date_id\"])\n",
    "        df_enriched.to_sql(\"fact_sales_enriched\", engine, if_exists=\"append\", index=False)\n",
    "        print(f\"Inserted final {len(batch_to_insert)} records\")\n",
    "    \n",
    "    print(f\"\\n{'*'*60}\")\n",
    "    print(f\"HYBRIDJOIN COMPLETED!\")\n",
    "    print(f\"Processed: {processed_count}, Joined: {joined_count}\")\n",
    "    print(f\"{'*'*60}\")\n",
    "\n",
    "# Start threads\n",
    "print(\"=\"*60)\n",
    "print(\"Starting HYBRID JOIN with Streaming\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stream_thread = threading.Thread(target=stream_producer, daemon=True)\n",
    "join_thread = threading.Thread(target=hybrid_join, daemon=True)\n",
    "\n",
    "stream_thread.start()\n",
    "time.sleep(2)\n",
    "join_thread.start()\n",
    "\n",
    "stream_thread.join()\n",
    "join_thread.join()\n",
    "\n",
    "print(\"\\nAll operations completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
